{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9668c7",
   "metadata": {},
   "source": [
    "## Co-occurencier \n",
    "Se munir du corpus de travail\n",
    "\n",
    "Ici deux méthodes sont développées et à prendre en compte\n",
    "\n",
    "1) Le corpus sous format .txt ne comprend pas d'informations concernant la pagination, il faut alors ignorer ce point et se concentrer sur un positionnement relatif des termes par rapport à la séance en entier\n",
    "2) Le corpus est numérisé mais pas encore océriser, il est alors assez facile de rajouter un petit marqueur spécial indiquant un changement de page (ex : \"===chgpage===\") qu'il est ensuite possible de détecter lors de l'ouverture du documents et nous permettrait plus facilement de retrouver cette information\n",
    "\n",
    "L'ORC le plus performant étant ici celui sans la pagination indiquée, le travail fut réaliser en suivant la première méthode et en indiquant néanmoins la date de la séance ainsi que la position relative de l'extrait\n",
    "\n",
    "Fournir deux listes de mots (ou utiliser celles fournies de base), listes pouvant être modifiées\n",
    "Il est également possible de travailler sur une unique liste afin de déterminer par exemple si les mots d'un même champ lexical sont employés dans les mêmes extraits ou non.\n",
    "\n",
    "Il est suggéré dans un premier temps d'utiliser le notebook recherchemotclefs.ipynb sur le corpus d'entrée afin de ne concerver uniquement les documents qui ont au moins une occurence des mots souhaités et ne pas travailler sur l'ensemble du corpus qui ne correspond pas forcément.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "documents à ouvrir: 100%|██████████| 7461/7461 [22:05<00:00,  5.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cooccurrences trouvées. Résultat exporté dans 'cooccurrences.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DOSSIER_TEXTES = \"C:/Users/portable_laura/Documents/coursM1/Mémoire/Sources/ocr_sorted\"      \n",
    "LISTE1_PATH = \"liste1.txt\"\n",
    "LISTE2_PATH = \"liste2.txt\"\n",
    "CONTEXT_SIZE = 10                    \n",
    "\n",
    "def lire_liste(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [mot.strip().lower() for mot in f if mot.strip()]\n",
    "\n",
    "def decouper_pages(texte):\n",
    "    texte = texte.lower()\n",
    "    texte = re.sub(r\"[^\\w\\s]\", \"\", texte) \n",
    "    return texte.split()\n",
    "\n",
    "def chercher_cooccurrences(tokens, liste1, liste2, doc, page_num, context_size=10):\n",
    "    resultats = []\n",
    "\n",
    "    for i, mot in enumerate(tokens):\n",
    "        if mot in liste1:\n",
    "            contexte_debut = max(0, i - context_size)\n",
    "            contexte_fin = min(len(tokens), i + context_size + 1)\n",
    "            contexte = tokens[contexte_debut:contexte_fin]\n",
    "            \n",
    "            for j, mot2 in enumerate(contexte):\n",
    "                if mot2 in liste2:\n",
    "                    if mot != mot2:\n",
    "                        distance = abs(i - (contexte_debut + j))\n",
    "                        extrait_contexte = \" \".join(contexte)\n",
    "                        resultats.append({\n",
    "                            \"mot1\": mot,\n",
    "                            \"mot2\": mot2,\n",
    "                            \"distance\": distance,\n",
    "                            \"contexte\": extrait_contexte,\n",
    "                            \"document\": doc,\n",
    "                            \"page\": page_num + 1\n",
    "                        })\n",
    "    return resultats\n",
    "\n",
    "#liste1 = lire_liste(LISTE1_PATH)\n",
    "#liste2 = lire_liste(LISTE2_PATH)\n",
    "liste1 =[\"freycinet\", \"gabarit\", \"Tardieu\",\n",
    "              \"halage\", \"curage\", \"dragage\", \"transbordement\", \"cabotage\",\n",
    "              \"irrigation\", \"adduction\", \"drainage\", \"canalisée\", \"mouillage\", \"canalisation\"\n",
    "              \"péniche\",  'bateau', \"bateaux\", \"gabare\", \"allège\", \"convoi\", \"automoteur\", \"chaland\", \n",
    "              'remorqueur', \"bac\", \"fret\", \"tonnage\",\n",
    "              \"tirant\",\"écluse\", 'fluvial', \"fleuve\", \"rivière\", \"affluent\",\"riviere\", \"canal\", \"canaux\",\n",
    "              \"bassin\", \"versant\", \"flux\",\n",
    "              'batellerie', 'navigable',\"navigation\", \"navigabilité\", \"pilotage\",\n",
    "              \"transport\", \"houille\",  \"marchandise\", \n",
    "              \"eau\", \"inondation\", \"barrage\",  \"chantier\",\n",
    "              \"loire\", \"oise\", \"garonne\", \"rhin\", \"rhône\", \"seine\", \"midi\",\"marne\",\n",
    "              \"ascenseur\", \"embarcation\", \"port\", \"quai\",\n",
    "              \"ponts\", \"chaussees\"]\n",
    "\n",
    "liste2 = [\"travaux\", \"investissement\", \"publics\", \"finances\", \"credit\", \"dotation\", \"budget\", \"dépense\",\"tresor\",\"utilite\",\n",
    "               \"amenagement\",\"infrastructure\",\"concession\",\"maitrise\",\"ouvrage\",\"questeur\",\"trésorier\",\"compagnie\"]\n",
    "\n",
    "\n",
    "cooc_finales = []\n",
    "\n",
    "for fichier in tqdm(os.listdir(DOSSIER_TEXTES),desc=\"documents à ouvrir\"):\n",
    "    if not fichier.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    chemin_complet = os.path.join(DOSSIER_TEXTES, fichier)\n",
    "    with open(chemin_complet, \"r\", encoding=\"utf-8\") as f:\n",
    "        texte = f.read()\n",
    "\n",
    "    pages = decouper_pages(texte)\n",
    "\n",
    "    for num_page, page in enumerate(pages):\n",
    "        tokens = decouper_pages(page)\n",
    "        cooc_page = chercher_cooccurrences(tokens, liste1, liste2, fichier, num_page, CONTEXT_SIZE)\n",
    "        cooc_finales.extend(cooc_page)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(cooc_finales)\n",
    "df = df.drop_duplicates(subset=[\"mot1\", \"mot2\", \"document\", \"page\", \"contexte\"])\n",
    "df.to_csv(\"C:/Users/portable_laura/Desktop/memoire_prod_finale/data/cooccurrences.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"{len(df)} cooccurrences trouvées. Résultat exporté dans 'cooccurrences.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a8b107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c30026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
