{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compte des coccurences\n",
      "['aart', 'ab', 'abaissement', 'abaisser', 'abaissé', 'abandon', 'abandonne', 'abandonner', 'abandonné', 'abattoir']\n",
      "[0, 0, 0, 0, 2, 0, 3, 0, 0, 0]\n",
      "Topic modeling\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "to_excel() got an unexpected keyword argument 'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 123\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m      \u001b[38;5;66;03m# calcul les bag of words pour chaque document, puis les ajoute dans le dataframe précédent\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     word_frequency \u001b[38;5;241m=\u001b[39m count_vectorizer(df, \u001b[38;5;241m10000\u001b[39m)  \u001b[38;5;66;03m# à partir des bag of words, calcul la matrice de fréquence par doc\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlda_model_blocs.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lance un modèle de LDA. Il ne fera rien si un modèle du même nom existe déjà\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     text_topics, topics, table_text_topics \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlda_model_blocs.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m     get_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlda_model_blocs.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 89\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(model_file)\u001b[0m\n\u001b[0;32m     85\u001b[0m all_topics: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: [clefs[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m top\u001b[38;5;241m.\u001b[39margsort()[\u001b[38;5;241m-\u001b[39mwords_per_topic:]]\n\u001b[0;32m     86\u001b[0m                                          \u001b[38;5;28;01mfor\u001b[39;00m i, top \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lda\u001b[38;5;241m.\u001b[39mcomponents_)})\n\u001b[0;32m     87\u001b[0m table_topics_to_texts: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28;01mlambda\u001b[39;00m z: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)(topic_to_text),\n\u001b[0;32m     88\u001b[0m                                                    columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(nb_topics), index\u001b[38;5;241m=\u001b[39mblocs)\n\u001b[1;32m---> 89\u001b[0m \u001b[43mall_topics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopics.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m table_topics_to_texts\u001b[38;5;241m.\u001b[39mto_excel(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorpus_topics.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: to_excel() got an unexpected keyword argument 'encoding'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle  # librairie pour save des modèle de machine learning\n",
    "import re\n",
    "from typing import List, TextIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer as FLF\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "lemmatizer = FLF()\n",
    "\n",
    "path_data = \"C:/Users/portable_laura/Documents/coursM1/Mémoire/Sources/ocr_sorted_av14/\"\n",
    "path_to_model = os.path.join(path_data, \"model_ML\")\n",
    "\n",
    "stop_words = set(list(open(os.path.join(path_data, \"french_stopwords.txt\"), \"r\", encoding=\"utf-8\").read().split(\"\\n\"))\n",
    "                 + stopwords.words(\"french\"))\n",
    "#Créer son txt de stop words, un par saut de ligne\n",
    "\n",
    "\n",
    "def table_ocr_1880():\n",
    "    print(\"récupération des fichiers\")\n",
    "    fichiers = os.listdir(path_data)\n",
    "    data = pd.DataFrame(columns=[\"date\", \"text\"])\n",
    "    for name in fichiers:\n",
    "        print(name)\n",
    "        if name.split(\"-\")[0][:2] == '18':\n",
    "            file: TextIO = open(os.path.join(path_data, name), encoding=\"utf-8\")\n",
    "            text: str = file.read()\n",
    "            file.close()\n",
    "            data = pd.concat([data, pd.DataFrame({'date': [name.split('.')[0]], \"text\": [text]})], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_corpus(df: pd.DataFrame):\n",
    "    print(\"nettoyage du texte\")\n",
    "    data: List = []\n",
    "    for i in range(df.shape[0]):\n",
    "        print(f\"{i}/{df.shape[0]}\")\n",
    "        text = \" \".join(re.findall(\"[A-Za-zâêûîôäëüïöùàçéèÉ\\-\\.]+\", df.text[i]))\n",
    "        text = re.sub(\"([a-z])- \", r\"\\1\", text)\n",
    "        text = re.sub(\"\\-\", \" \", text)\n",
    "        text = re.sub(\"[M]+\\. ([A-Z]+[a-zâêûîôäëüïöùàçéè]+(?:\\s[A-Z]+[a-zâêûîôäëüïöùàçéè]+)?)\", \" \", text)\n",
    "        text = re.sub(\"\\.\", \" \", text)\n",
    "        bag_of_words: List[str] = word_tokenize(text.lower(), language=\"french\")\n",
    "        bag_of_words = [w for w in bag_of_words if 1 <= len(w) < 22]\n",
    "        bag_of_words = [lemmatizer.lemmatize(w).lower() for w in bag_of_words if w not in stop_words]\n",
    "        data.append(bag_of_words)\n",
    "    df.loc[:, \"bag_of_words\"] = data\n",
    "    return df\n",
    "\n",
    "\n",
    "def count_vectorizer(df: pd.DataFrame, p: int):\n",
    "    print(\"compte des coccurences\")\n",
    "    data = [\" \".join(w) for w in df.bag_of_words]\n",
    "    vectorizer = CountVectorizer(max_features=p)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    word_frequency_matrix = pd.DataFrame(data=X.toarray(), index=df.date, columns=vectorizer.get_feature_names_out())\n",
    "    word_frequency_matrix = word_frequency_matrix.sort_index()\n",
    "    word_frequency_matrix.to_csv(os.path.join(path_data, \"word_frequency_80.csv\"),\n",
    "                                 sep=\";\", encoding=\"utf-8\", index=False)\n",
    "    return word_frequency_matrix\n",
    "\n",
    "def build_model(model_file):\n",
    "    nb_topics: int = 50\n",
    "    words_per_topic: int = 20\n",
    "    if os.path.exists(os.path.join(path_to_model, model_file)):\n",
    "        print(\"il existe déjà un modèle avec ce nom\")\n",
    "    else:\n",
    "        word_frequency_matrix = pd.read_csv(os.path.join(path_data, \"word_frequency_80.csv\"), sep=\";\", encoding=\"utf-8\",\n",
    "                                            index_col=0)\n",
    "        clefs: List[str] = list(word_frequency_matrix.columns)\n",
    "        print(clefs[:10])\n",
    "        blocs: List[str] = list(word_frequency_matrix.index)\n",
    "        print(blocs[:10])\n",
    "        print(\"Topic modeling\")\n",
    "        lda = LatentDirichletAllocation(n_components=nb_topics)\n",
    "        topic_to_text = lda.fit_transform(word_frequency_matrix.values)\n",
    "        pkl_filename = os.path.join(path_to_model, model_file)\n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(lda, file)\n",
    "    all_topics: pd.DataFrame = pd.DataFrame({f\"Topic{i}\": [clefs[w] for w in top.argsort()[-words_per_topic:]]\n",
    "                                             for i, top in enumerate(lda.components_)})\n",
    "    table_topics_to_texts: pd.DataFrame = pd.DataFrame(np.vectorize(lambda z: f\"{z:.3f}\")(topic_to_text),\n",
    "                                                       columns=range(nb_topics), index=blocs)\n",
    "    all_topics.to_excel(os.path.join(path_data, \"topics.xlsx\"), encoding=\"utf-8\", index=False)\n",
    "    table_topics_to_texts.to_excel(os.path.join(path_data, \"corpus_topics.xlsx\"), encoding=\"utf-8\", index=True)\n",
    "\n",
    "\n",
    "def load_model(model_file):\n",
    "    word_freq = pd.read_csv(os.path.join(path_data, \"word_frequency_80.csv\"), sep=\";\", encoding=\"utf-8\", index_col=0)\n",
    "    clefs: List[str] = list(word_freq.columns)\n",
    "    blocs: List[str] = list(word_freq.index)\n",
    "    pkl_filename = os.path.join(path_to_model, model_file)\n",
    "    with open(pkl_filename, 'rb') as file:\n",
    "        lda = pickle.load(file)\n",
    "    nb_topics: int = lda.n_components\n",
    "    words_per_topic: int = 20\n",
    "    topic_to_text = lda.transform(word_freq.values)\n",
    "    all_topics: pd.DataFrame = pd.DataFrame({f\"Topic{i}\": [clefs[w] for w in top.argsort()[-words_per_topic:]]\n",
    "                                             for i, top in enumerate(lda.components_)})\n",
    "    table_topics_to_texts: pd.DataFrame = pd.DataFrame(np.vectorize(lambda z: f\"{z:.3f}\")(topic_to_text),\n",
    "                                                       columns=range(nb_topics), index=blocs)\n",
    "    all_topics.to_excel(os.path.join(path_data, \"topics.xlsx\"), encoding=\"utf-8\", index=False)\n",
    "    table_topics_to_texts.to_excel(os.path.join(path_data, \"corpus_topics.xlsx\"), encoding=\"utf-8\", index=True)\n",
    "    return topic_to_text, all_topics, table_topics_to_texts\n",
    "\n",
    "\n",
    "def get_parameter(adr: str):\n",
    "    pkl_filename = os.path.join(path_to_model, adr)\n",
    "    with open(pkl_filename, 'rb') as file:\n",
    "        lda: LatentDirichletAllocation = pickle.load(file)\n",
    "    print(lda.get_params())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "     # calcul les bag of words pour chaque document, puis les ajoute dans le dataframe précédent\n",
    "    word_frequency = count_vectorizer(df, 10000)  # à partir des bag of words, calcul la matrice de fréquence par doc\n",
    "    build_model('lda_model_blocs.pkl')  # lance un modèle de LDA. Il ne fera rien si un modèle du même nom existe déjà\n",
    "    text_topics, topics, table_text_topics = load_model(\"lda_model_blocs.pkl\")\n",
    "    get_parameter(\"lda_model_blocs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table_ocr_1880()  # collecte dans un dataframe, les textes des années 80 avec la date comme référence\n",
    "df = build_corpus(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_avance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
