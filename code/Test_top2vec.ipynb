{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Essai word Embedding avec Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\portable_laura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\portable_laura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des documents - fonction\n",
    "def ouverture_documents(chemin_dossier): \n",
    "\n",
    "    liste_fichiers= os.listdir(chemin_dossier)\n",
    "    liste_retour=[]\n",
    "    for fichier in tqdm(liste_fichiers) :\n",
    "        with open(os.path.join(chemin_dossier,fichier),'r',encoding=\"utf-8\") as texte:\n",
    "            seance=texte.read()\n",
    "        liste_retour.append(seance)\n",
    "    return liste_retour\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7461/7461 [05:43<00:00, 21.73it/s]\n"
     ]
    }
   ],
   "source": [
    "#Importation des documents - exécution\n",
    "chemin = \"C:/Users/portable_laura/Documents/coursM1/Mémoire/Sources/ocr_sorted\"\n",
    "liste_desdocs = ouverture_documents(chemin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing des documents - fonction\n",
    "\n",
    "\n",
    "def preprocess(doc_texte):\n",
    "    doc_texte = doc_texte.lower()\n",
    "    doc_texte = re.sub(r'[^\\w\\sÀ-ÿ]', ' ', doc_texte)  # Utilisation de la plage Unicode pour inclure les caractères français\n",
    "    doc_texte = re.sub(r'\\d+', ' ', doc_texte)\n",
    "    words = word_tokenize(doc_texte, language='french')  # Tokenization pour le français\n",
    "    \n",
    "    words = [word for word in words if word not in stopwords.words('french')]  # Utilisation de mots vides en français, cette liste est à modifier et a agrémenter des mots vides correspondant au corpus\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une première expérimentation de tokénisation qui prend l'ensemble du corpus en une seule exécution, le temps d'exécution m'a permis de réaliser l'empleur du corpus et la charge calculatoire qu'il était nécessaire d'accorder pour pouvoir le considérer dans son intégralité \n",
    "\n",
    "```liste_preprocess = []```     \n",
    "```for texte in tqdm(liste_desdocs):```  \n",
    "    ```texte_prepro = preprocess(texte)```  \n",
    "    ```liste_preprocess.append(texte_prepro)```  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faire gaffe à ne pas tout effacer\n",
    "#liste_preprocess_etape = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxième tentative de prétraitement du corpus, une première hypothèse fut que le temps de calcul sur la cellule qui exécutait l'intégralité de la tokénisation sur les données s'essoufflait à force de boucler et enregistrer les tokens en continu, j'ai donc tenté de découper le corpus en segments abordables en enregistrant progressivement les données. Cela fonctionne mais prend encore une fois énormément de temps. J'ai effectué ceci jusqu'au 4300 document, ce qui correspond au début des années 1910, borne à laquelle je voulais effectuer mes premiers tests avant de poursuivre sur le reste du corpus. \n",
    "Une autre méthode fut testé pour l'extraction des données par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/299 [00:23<1:57:03, 23.57s/it]"
     ]
    }
   ],
   "source": [
    "for numero_texte in tqdm(range(4001,4300)):\n",
    "    texte_prepro = preprocess(liste_desdocs[numero_texte])\n",
    "    liste_preprocess.append(texte_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7461"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(liste_preprocess))\n",
    "len(liste_desdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enregistrement liste de liste de mots 1 à 3500\n",
    "\n",
    "with open('3500premiers_debats_preprocess.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(liste_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_csv = '3500premiers_debats_preprocess.csv'\n",
    "\n",
    "liste_preprocess = []\n",
    "\n",
    "with open(fichier_csv, mode='r', newline='', encoding='utf-8') as fichier:\n",
    "    lecteur_csv = csv.reader(fichier)\n",
    "    \n",
    "    for ligne in lecteur_csv:\n",
    "        liste_preprocess.append(ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test réduction de corpus, échantillonage aléatoire\n",
    "import random\n",
    "\n",
    "def extract_random_substrings(strings_list, substring_length=3000):\n",
    "    extracted_substrings = []\n",
    "\n",
    "    for s in strings_list:\n",
    "        length = len(s)\n",
    "        if length <= substring_length:\n",
    "            extracted_substrings.append(s)\n",
    "        else:\n",
    "            start_index = random.randint(0, length - substring_length)\n",
    "            extracted_substrings.append(s[start_index:start_index + substring_length])\n",
    "    \n",
    "    return extracted_substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7461"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_extraits_alea = extract_random_substrings(liste_desdocs, random_seed = 42)\n",
    "len(liste_extraits_alea)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/portable_laura/Documents/coursM1/Mémoire/Code/extraits_aleatoires_corpus_memoire.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    export = csv.writer(f)\n",
    "    for document in liste_extraits_alea :\n",
    "        export.writerow([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7461/7461 [24:49<00:00,  5.01it/s] \n"
     ]
    }
   ],
   "source": [
    "liste_preprocess_sur_alea = []\n",
    "for texte in tqdm(liste_extraits_alea):  \n",
    "    texte_prepro = preprocess(texte)\n",
    "    liste_preprocess_sur_alea.append(texte_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/portable_laura/Documents/coursM1/Mémoire/Code/preprocess_sur_alea.csv', 'w', newline='') as file:\n",
    "    document = csv.writer(file)\n",
    "    document.writerows(liste_preprocess_sur_alea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_avance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
